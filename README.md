# webautomation-scraping
Daily Report Scraper
This project automates the login, navigation, data extraction, and Excel update process platform's back office. It uses Selenium and BeautifulSoup to scrape daily win/loss data and updates a predefined Excel report.

ğŸš€ Features
âœ… Automated login via Selenium

âœ… Dynamic date picker interaction

âœ… Data scraping using BeautifulSoup

âœ… Excel file manipulation using openpyxl

âœ… Simple total calculation (Bet Amount & Payout)

âœ… Clean output formatting for Jupyter Notebooks

âœ… Uses dynamic file naming and row targeting

ğŸ§° Technologies Used
Python 3.9+ â€“ Core programming language
Selenium â€“ Browser automation and web interaction
BeautifulSoup (bs4) â€“ HTML parsing and table data extraction
pandas â€“ Date handling and basic data transformation
openpyxl â€“ Excel file reading and writing
IPython â€“ Enhanced notebook display (Markdown rendering)
SOCKS5 Proxy Support (Optional) â€“ Route browser traffic through a local proxy using:
Jupyter Notebook or any Python runtime

ğŸ”’ Sensitive Data & Config
This project intentionally excludes:
Usernames
Passwords
API links
Proxy addresses
Local Excel paths

To keep this project secure and reusable, all sensitive data is expected to be loaded from a separate config or .env file, or handled inside an ignored notebook

ğŸ“ Usage
Modify start_date, end_date, and filenamedt as needed.

Run the script in Jupyter or any Python environment.

The output Excel will be saved to the defined path with updated bet and payout values.

 Security Best Practices
Do not commit real credentials or internal URLs.

Add all personal notebooks and .env files to .gitignore.

Use placeholder values when sharing public examples.
